<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Cui Chi</title><meta name="author" content="Cui Chi"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 5.4.2"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Cui Chi</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/blogs"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/profile.png" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Cui Chi</h3><p class="author-bio">Master student in NUS</p></div><div class="author-links"><button class="btn m-social-links">Links</button></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a><a target="_blank" rel="noopener" href="https://clustrmaps.com/site/1bvca"><img src="//clustrmaps.com/map_v2.png?cl=ffffff&amp;w=a&amp;t=tt&amp;d=1-ZWvsyA2_8iEeKv0taNJbg0D67qsg-52quih6hCUC8" style="display: none"></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title"></h2><article><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>This article presents a novel side channel attack strategy for extracting data from machine learning models, such as the model type, optimization algorithm, and batch size, leveraging scientific plots like t-SNE and Loss plots.</p>
<p><strong>Methodology:</strong> The authors utilize plots derived from shadow models to train an attack model, which can predict certain attributes of the original model. The process unfolds in three steps: Firstly, the attacker randomly selects information to generate a variety of shadow models. Secondly, they create scientific plots for each shadow model. Lastly, these plots, alongside model information, serve as inputs to train an attack model, which is essentially an image classifier.</p>
<p><strong>Evaluation:</strong> The method’s effectiveness is assessed on both t-SNE plots and loss plots.</p>
<p><strong>Evaluation on t-SNE Plots:</strong> For t-SNE plots, the performance is evaluated not only on the model type, optimization algorithm, and batch size, but also on custom model architecture such as the number of convolution layers. The results demonstrate that the proposed technique successfully infers model information, particularly the optimization algorithm, using t-SNE plots. It can also accurately predict activation functions, the number of fully connected layers, and convolutional layers in custom model architectures.</p>
<p>Following this, an ablation study is conducted, revealing that the attack remains effective even with a limited number of shadow models and is not significantly impacted by the color, density, and perplexity of the t-SNE plots. The attack can still extract model information even when t-SNE plots are generated from different dataset distributions, density, and perplexity settings. The authors then compare their attack to query-based model parameter stealing attacks and discover that their approach matches up to, and even surpasses these when the query output of the target model is limited. The attack is also applied to downstream adversarial attacks, proving that it enhances the efficiency of creating an adversarial model by identifying a shadow model similar to the target.</p>
<p>Lastly, several defenses against the attack are examined. The two most effective methods discovered involve thresholding embedding values and introducing Gaussian noise into the coordinates of the t-SNE points. However, these defenses are invalidated when the attacker is aware of them.</p>
<p><strong>Evaluation on Loss Plots:</strong> For loss plots, the attack performance generally outperforms that on t-SNE plots. A comparison of loss plots with and without axes shows that the loss curve itself is instrumental in enabling the attack model to identify model types. Additionally, three defensive strategies are tested, with TensorBoard and sliding window proving to be effective. However, these strategies fail in the presence of an adaptive attacker.</p>
<p><strong>Analysis:</strong> The authors leverage Grad-CAM to understand why scientific plots can infer model information. The findings suggest that the shape of the t-SNE plots and the first 10 timestamps of the loss plots most significantly impact the original model’s performance, indicating potential data leakage.</p>
<p><strong>Limitations:</strong></p>
<ul>
<li>The approach assumes knowledge of the training dataset distribution. In practice, most attacks are black-box, meaning the adversary lacks information about both the model and the dataset. Performance significantly decreases if shadow models and target models are trained with different datasets. Without knowledge of the target model’s training dataset, the attack strategy becomes significantly challenging.</li>
<li>The attack relies on scientific plots, which may not always be readily available from all target models.</li>
</ul>
<p><strong>Conclusion:</strong> In summary, the article introduces a unique technique for using scientific plots to reveal information about target models, offering a non-interactive means of model data theft. However, its practical application is constrained by the requirement for adversaries to know the training dataset’s distribution. Consequently, for industry models where both the architecture and dataset are private, this approach falls short.</p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/blogs"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><script type="text/javascript" src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_site_pv">Hits: <span id="busuanzi_value_site_pv"><i class="fa fa-spinner fa-spin"></i></span></span><div class="copyright">&copy;2020 - 2023 by Cui Chi</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" async>$(document).on({
  'pjax:complete': function() {
    $.ajax({
      type: "GET",
      url: "https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-MML-AM_CHTML",
      dataType: "script",
      cache: true,
      success: function() {
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
        var math = document.getElementsByClassName("entry-content")[0];
        MathJax.Hub.Queue(["Typeset",MathJax.Hub,math]);
      },
    });
  }
});</script></body></html>